#pragma kernel ApplyLaplacianAndDot
#pragma kernel Axpy // y = a*x + y
#pragma kernel Scale // y = a*y
#pragma kernel CalculateDivergence
#pragma kernel DotProduct
#pragma kernel ApplySparseGAndDot
#pragma kernel ApplySparseGT
#pragma kernel ClearBufferFloat
#pragma kernel PrecomputeIndices
#pragma kernel ComputeDiagonal
#pragma kernel ApplyJacobi

// Include common HLSL functions
#include "Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl"

// Struct definitions (must match C# structs)
struct faceVelocities
{
    float left;
    float right;
    float bottom;
    float top;
    float front;
    float back;
};

struct Node
{
    float3 position;    // 12 bytes
    float3 velocity;    // 12 bytes
    faceVelocities velocities;    // 24 bytes
    float mass;          // 4 bytes
    uint layer;         // 4 bytes
    uint mortonCode;    // 4 bytes
    uint active;        // 4 bytes
};

// Buffers for CG solver
RWStructuredBuffer<Node> nodesBuffer;
RWStructuredBuffer<float> divergenceBuffer;
RWStructuredBuffer<uint> neighborsBuffer;
StructuredBuffer<uint> reverseNeighborsBuffer; // Reverse neighbor slot indices per node (numNodes * 24)
RWStructuredBuffer<float> pBuffer; // The input vector 'p'
RWStructuredBuffer<float> ApBuffer; // The output vector 'Ap'
RWStructuredBuffer<float> xBuffer;
RWStructuredBuffer<float> yBuffer;
RWStructuredBuffer<float> matrixGBuffer; // [N * 25] The Preconditioner Matrix G (0=Diag, 1-24=Neighbor Slots)
RWStructuredBuffer<float> zBuffer;       // Intermediate buffer "u" and Output "z"
RWStructuredBuffer<uint> scatterIndicesBuffer; // NEW: [N * 24] Pre-computed scatter indices
uint numNodes;
float a; // scalar 'a'
float deltaTime; // Time step for physics scaling

// Shared memory for dot product reduction
groupshared float s_data[512];

[numthreads(512, 1, 1)]
void CalculateDivergence(uint3 id : SV_DispatchThreadID)
{
    uint i = id.x;
    if (i >= numNodes) return;

    Node node = nodesBuffer[i];
    faceVelocities v = node.velocities;

    // Calculate Net Flux of Velocity
    // Flux = Sum(Velocity * FaceArea)
    float dx = exp2((float)node.layer);
    float faceArea = dx * dx;
    
    // Net flux leaving the cell
    float netFlux = (v.right - v.left + v.top - v.bottom + v.back - v.front) * faceArea;
    
    // We solve Ap = b. If A is negative Laplacian, b should be negative Divergence.
    float outVal = -netFlux;
    
    if (!isfinite(outVal)) outVal = 0.0;
    divergenceBuffer[i] = outVal;
}


// FUSED KERNEL: ApplyLaplacian + DotProduct
// Computes Ap = A * p AND p · Ap in a single pass to reduce memory round-trips
groupshared float s_dot_reduction[256];

// --- OPTIMIZED LDS KERNEL ---
// Shared memory to cache node data for the entire thread group
groupshared Node s_nodes[256];
groupshared float s_p_values[256];

[numthreads(256, 1, 1)]
void ApplyLaplacianAndDot(uint3 Gid : SV_GroupID, uint3 Gtid : SV_GroupThreadID, uint3 id : SV_DispatchThreadID)
{
    uint i = id.x;
    uint li = Gtid.x; // Local index [0..255]
    uint group_start_idx = Gid.x * 256;

    // --- Phase 1: Collaborative Load into LDS ---
    // ALL threads must participate in the sync, so perform load within a bounds check
    if (i < numNodes)
    {
        s_nodes[li] = nodesBuffer[i];
        s_p_values[li] = pBuffer[i];
    }
    // Sync to ensure all data is loaded before proceeding. ALL threads hit this.
    GroupMemoryBarrierWithGroupSync();

    float p_i = 0.0;
    float Ap_val = 0.0;
    
    // --- Phase 2: Main Calculation (for valid threads) ---
    // NO early return. Inactive threads will contribute 0 to the reduction.
    if (i < numNodes)
    {
        // Read own data from fast LDS
        Node node = s_nodes[li];
        p_i = s_p_values[li];
        float laplacian_p = 0.0;

        if (!isfinite(p_i)) p_i = 0.0;

        float dx_i = exp2((float)node.layer);
        if (!isfinite(dx_i)) dx_i = 1e-6;

        // Loop over all 6 faces of the node
        for (int d = 0; d < 6; d++)
        {
            uint faceSlotBase = d * 4;
            uint neighbor_idx = neighborsBuffer[faceSlotBase * numNodes + i];

            // Case 1: True Boundary (Neumann BC)
            if (neighbor_idx == numNodes + 1)
            {
                laplacian_p += 0.0;
            }
            // This global read is required to decide the logic path
            else if (neighbor_idx < numNodes && nodesBuffer[neighbor_idx].layer >= node.layer)
            {
                // Case 2: Same-layer or Parent neighbor
                Node nNode;
                float p_j;

                // --- CACHE CHECK ---
                if (neighbor_idx >= group_start_idx && neighbor_idx < group_start_idx + 256)
                {
                    // Cache Hit: Read from LDS
                    uint local_neighbor_idx = neighbor_idx - group_start_idx;
                    nNode = s_nodes[local_neighbor_idx];
                    p_j = s_p_values[local_neighbor_idx];
                }
                else
                {
                    // Cache Miss: Read from Global Memory
                    nNode = nodesBuffer[neighbor_idx];
                    p_j = pBuffer[neighbor_idx];
                }

                float dx_j = exp2((float)nNode.layer);
                float distance = max(0.5 * (dx_i + dx_j), 1e-6);
                float a_shared = dx_i * dx_i;
                float w = a_shared / distance;
                float contribution = w * (p_j - p_i);
                
                laplacian_p += contribution;
            }
            else
            {
                // Case 3/4: Child neighbors or Dirichlet boundary
                float faceArea = dx_i * dx_i;
                float childFaceArea = faceArea / 4.0;
                
                for (int k = 0; k < 4; k++)
                {
                    uint child_idx = neighborsBuffer[(faceSlotBase + k) * numNodes + i];
                    
                    if (child_idx < numNodes) // Case 3: Child neighbor
                    {
                        Node cNode;
                        float p_k;

                        // --- CACHE CHECK ---
                        if (child_idx >= group_start_idx && child_idx < group_start_idx + 256)
                        {
                            // Cache Hit
                            uint local_child_idx = child_idx - group_start_idx;
                            cNode = s_nodes[local_child_idx];
                            p_k = s_p_values[local_child_idx];
                        }
                        else
                        {
                            // Cache Miss
                            cNode = nodesBuffer[child_idx];
                            p_k = pBuffer[child_idx];
                        }
                        
                        float dx_k = exp2((float)cNode.layer);
                        float distance = max(0.5 * (dx_i + dx_k), 1e-6);
                        float w = childFaceArea / distance;
                        float contribution = w * (p_k - p_i);
                        laplacian_p += contribution;
                    }
                    else if (child_idx == numNodes) // Case 4: Dirichlet boundary
                    {
                        float dx_k = dx_i * 0.5;
                        float distance = max(0.5 * (dx_i + dx_k), 1e-6);
                        float w = childFaceArea / distance;
                        float contribution = w * (-p_i);
                        laplacian_p += contribution;
                    }
                }
            }
        }

        // --- Return Net Flux (not volume-normalized) ---
        // KEEP this (assuming negative sign is desired for the operator)
        float outVal = -laplacian_p; 
        
        // Apply time step if needed, though typically handled in the update step
        outVal *= deltaTime; 

        if (!isfinite(outVal)) outVal = 0.0;
        Ap_val = outVal;
        
        // Write Ap to memory (still needed for next steps)
        ApBuffer[i] = Ap_val;
    } // End of "if (i < numNodes)"

    // 3. FUSED DOT PRODUCT (p . Ap) -- ALL threads participate
    float product = p_i * Ap_val;
    if (!isfinite(product)) product = 0.0;
    
    // Standard Parallel Reduction
    s_dot_reduction[li] = product;
    GroupMemoryBarrierWithGroupSync();
    
    for (uint s = 128; s > 0; s >>= 1) {
        if (li < s) {
            float sum = s_dot_reduction[li] + s_dot_reduction[li + s];
            if (!isfinite(sum)) sum = 0.0;
            s_dot_reduction[li] = sum;
        }
        GroupMemoryBarrierWithGroupSync();
    }
    
    // Write partial sum for this group
    if (li == 0) {
        // We reuse divergenceBuffer as scratch space for dot product reductions
        // just like the standalone DotProduct kernel did
        float result = s_dot_reduction[0];
        if (!isfinite(result)) result = 0.0;
        divergenceBuffer[Gid.x] = result;
    }
}

[numthreads(512, 1, 1)]
void Scale(uint3 id : SV_DispatchThreadID) {
    uint i = id.x;
    if (i >= numNodes) return;
    
    float y_val = yBuffer[i];
    
    float result = a * y_val;
    
    yBuffer[i] = result;
}

[numthreads(512, 1, 1)]
void Axpy(uint3 id : SV_DispatchThreadID) {
    uint i = id.x;
    if (i >= numNodes) return;
    
    float x_val = xBuffer[i];
    float y_val = yBuffer[i];
    
    float result = a * x_val + y_val;
    
    yBuffer[i] = result;
}

// For Dot Product (a simple, multi-pass reduction)
[numthreads(512, 1, 1)]
void DotProduct(uint3 Gid : SV_GroupID, uint3 Gtid : SV_GroupThreadID, uint3 id : SV_DispatchThreadID) {
    uint i = id.x;
    uint li = Gtid.x;

    float x_val = (i < numNodes) ? xBuffer[i] : 0.0;
    float y_val = (i < numNodes) ? yBuffer[i] : 0.0;
    
    // Guard against NaN values
    if (!isfinite(x_val)) x_val = 0.0;
    if (!isfinite(y_val)) y_val = 0.0;
    
    float product = x_val * y_val;
    if (!isfinite(product)) product = 0.0;
    
    s_data[li] = product;
    GroupMemoryBarrierWithGroupSync();

    for (uint s = 256; s > 0; s >>= 1) {
        if (li < s) {
            float sum = s_data[li] + s_data[li + s];
            if (!isfinite(sum)) sum = 0.0;
            s_data[li] = sum;
        }
        GroupMemoryBarrierWithGroupSync();
    }

    if (li == 0) {
        float result = s_data[0];
        if (!isfinite(result)) result = 0.0;
        divergenceBuffer[Gid.x] = result;
    }
}

// --------------------------------------------------------------------------------
// NEURAL PRECONDITIONER KERNELS
// Implements M^-1 * r approx (G * G^T + eps * I) * r
// --------------------------------------------------------------------------------

// --- ADD THIS KERNEL ---
// Pre-computes scatter indices to optimize ApplySparseGT
[numthreads(512, 1, 1)]
void PrecomputeIndices(uint3 id : SV_DispatchThreadID)
{
    uint i = id.x;
    if (i >= numNodes) return;

    // Pre-calculate indices for the Scatter (GT) operation
    for (int n = 0; n < 24; n++)
    {
        // SoA layout: neighborsBuffer[slot * numNodes + nodeIndex]
        uint k = neighborsBuffer[n * numNodes + i];
        // If neighbor is valid, store index. If not, store Sentinel (0xFFFFFFFF)
        // SoA layout: scatterIndicesBuffer[slot * numNodes + nodeIndex]
        scatterIndicesBuffer[n * numNodes + i] = (k < numNodes) ? k : 0xFFFFFFFF;
    }
}

// OPTIMIZED GATHER KERNEL (Replaces Scatter)
// Computes u = G^T * r
// Formula: u_i = G_ii * r_i + Sum_{k in neighbors} ( G_ki * r_k )
// G is now size [N, 7] where 0=Diag, 1-6=Face Neighbors
[numthreads(512, 1, 1)]
void ApplySparseGT(uint3 id : SV_DispatchThreadID)
{
    uint i = id.x;
    if (i >= numNodes) return;

    float r_i = xBuffer[i];
    if (!isfinite(r_i)) r_i = 0.0;

    // 1. Diagonal
    // SoA layout: matrixGBuffer[dim * numNodes + nodeIndex]
    float g_diag = matrixGBuffer[i]; // dim=0, so index is just i
    if (!isfinite(g_diag)) g_diag = 0.0;
    float sum = g_diag * r_i;

    // 2. Off-Diagonal (Optimized)
    for (int n = 0; n < 24; n++)
    {
        // SoA layout: scatterIndicesBuffer[slot * numNodes + nodeIndex]
        uint k = scatterIndicesBuffer[n * numNodes + i];

        // 0xFFFFFFFF is uint.MaxValue, check for valid neighbor
        if (k != 0xFFFFFFFF)
        {
            // Note: reverseNeighborsBuffer can stay AoS for now since it's rarely accessed
            uint revSlot = reverseNeighborsBuffer[i * 24 + n];
            if (revSlot < 24) 
            {
                // SoA layout: matrixGBuffer[dim * numNodes + nodeIndex]
                // We need coeff at node 'k', slot 'revSlot'.
                // Formula: (SlotIndex) * numNodes + NodeIndex
                // SlotIndex = 1 + revSlot (since 0 is diagonal)
                // NodeIndex = k
                float g_ki = matrixGBuffer[(1 + revSlot) * numNodes + k];
                
                if (!isfinite(g_ki)) g_ki = 0.0;
                float r_k = xBuffer[k];
                if (!isfinite(r_k)) r_k = 0.0;
                
                sum += g_ki * r_k;
            }
        }
    }

    if (!isfinite(sum)) sum = 0.0;
    zBuffer[i] = sum;
}


// FUSED KERNEL: ApplySparseG + DotProduct
// Computes z = G * u + eps * r AND r · z in a single pass to reduce memory round-trips
groupshared float s_precon_dot[256];
groupshared float s_z_values[256]; // Cache for vector 'u' (zBuffer)

[numthreads(256, 1, 1)] // Changed from 512 to 256
void ApplySparseGAndDot(uint3 Gid : SV_GroupID, uint3 Gtid : SV_GroupThreadID, uint3 id : SV_DispatchThreadID)
{
    uint i = id.x;
    uint li = Gtid.x;
    uint group_start_idx = Gid.x * 256;

    // --- 1. Collaborative Load into LDS ---
    // ALL threads must participate.
    if (i < numNodes) {
        s_z_values[li] = zBuffer[i];
    }
    GroupMemoryBarrierWithGroupSync(); // Wait for all threads in group to finish loading

    float z_val = 0.0;
    float r_val = 0.0;

    if (i < numNodes)
    {
        // Read own value from fast LDS
        float u_val = s_z_values[li];
        if (!isfinite(u_val)) u_val = 0.0;

        r_val = xBuffer[i]; // Residual r (needed for epsilon skip and dot product)
        if (!isfinite(r_val)) r_val = 0.0;
        
        float z_sum = 0.0;
        
        // A. Diagonal
        float g_diag = matrixGBuffer[i];
        if (!isfinite(g_diag)) g_diag = 0.0;
        z_sum += u_val * g_diag;

        // B. Neighbors (Optimized Gather)
        for(int k=0; k<24; k++)
        {
            uint n_idx = neighborsBuffer[k * numNodes + i];
            if(n_idx < numNodes)
            {
                float g_coeff = matrixGBuffer[(k + 1) * numNodes + i];
                if (!isfinite(g_coeff)) g_coeff = 0.0;

                float u_neighbor;
                
                // --- LDS CACHE CHECK ---
                if (n_idx >= group_start_idx && n_idx < group_start_idx + 256)
                {
                    // Cache Hit: Read from Shared Memory (Fast)
                    u_neighbor = s_z_values[n_idx - group_start_idx];
                }
                else
                {
                    // Cache Miss: Read from Global Memory (Slow)
                    u_neighbor = zBuffer[n_idx];
                }

                if (!isfinite(u_neighbor)) u_neighbor = 0.0;
                z_sum += g_coeff * u_neighbor;
            }
        }

        // C. Epsilon Regularization
        z_sum += 1e-4 * r_val;
        
        if (!isfinite(z_sum)) z_sum = 0.0;
        
        z_val = z_sum;
        
        // Write Result z to memory (needed for p update later)
        yBuffer[i] = z_val;
    }

    // --- 2. FUSED DOT PRODUCT (r . z) ---
    float product = r_val * z_val;
    if (!isfinite(product)) product = 0.0;
    
    // Parallel Reduction in Shared Memory
    s_precon_dot[li] = product;
    GroupMemoryBarrierWithGroupSync();
    
    // Reduction loop for 256 threads
    for (uint s = 128; s > 0; s >>= 1) {
        if (li < s) {
            float sum = s_precon_dot[li] + s_precon_dot[li + s];
            if (!isfinite(sum)) sum = 0.0;
            s_precon_dot[li] = sum;
        }
        GroupMemoryBarrierWithGroupSync();
    }
    
    // Write partial reduction to global memory
    if (li == 0) {
        // Reuse divergenceBuffer as scratchpad
        float result = s_precon_dot[0];
        if (!isfinite(result)) result = 0.0;
        divergenceBuffer[Gid.x] = result;
    }
}

// KERNEL 3: Clear Buffer (Helper to zero out zBuffer before Scatter)
[numthreads(512, 1, 1)]
void ClearBufferFloat(uint3 id : SV_DispatchThreadID)
{
    uint i = id.x;
    if (i < numNodes) zBuffer[i] = 0.0;
}

// KERNEL 4: Compute Diagonal of Laplacian Matrix A
// Computes the diagonal entry A_ii for each node i
// The diagonal is the negative sum of all off-diagonal weights
RWStructuredBuffer<float> diagonalBuffer; // Output buffer for diagonal values
[numthreads(512, 1, 1)]
void ComputeDiagonal(uint3 id : SV_DispatchThreadID)
{
    uint i = id.x;
    if (i >= numNodes) return;

    Node node = nodesBuffer[i];
    float dx_i = exp2((float)node.layer);
    if (!isfinite(dx_i)) dx_i = 1e-6;
    
    float diagonal_sum = 0.0;

    // Loop over all 6 faces of the node
    for (int d = 0; d < 6; d++)
    {
        uint faceSlotBase = d * 4; // Base slot for this face (0, 4, 8, 12, 16, 20)
        // SoA layout: neighborsBuffer[slot * numNodes + nodeIndex]
        uint neighbor_idx = neighborsBuffer[faceSlotBase * numNodes + i]; // first slot: same/parent layer

        // Case 1: True Boundary (Neumann BC, neighbor_idx == numNodes + 1)
        if (neighbor_idx == numNodes + 1)
        {
            // Neumann BC means zero flux (dp/dn = 0).
            // No contribution to diagonal.
        }
        // Case 2: Same-layer or Parent (coarser) neighbor
        else if (neighbor_idx < numNodes && nodesBuffer[neighbor_idx].layer >= node.layer)
        {
            Node nNode = nodesBuffer[neighbor_idx];
            float dx_j = exp2((float)nNode.layer);
            
            // Distance between cell centers
            float distance = max(0.5 * (dx_i + dx_j), 1e-6);
            
            // Face area is the area of cell i's face (dx_i^2)
            float a_shared = dx_i * dx_i;
            
            // Weight: area divided by distance
            float w = a_shared / distance;
            
            diagonal_sum += w;
        }
        else
        {
            // Case 3 or 4: Child (finer) neighbors or Dirichlet boundary
            // The face of cell i is subdivided into 4 child cells
            float faceArea = dx_i * dx_i;
            float childFaceArea = faceArea / 4.0; // Each child occupies 1/4 of the face
            
            for (int k = 0; k < 4; k++)
            {
                // SoA layout: neighborsBuffer[slot * numNodes + nodeIndex]
                uint child_idx = neighborsBuffer[(faceSlotBase + k) * numNodes + i];
                // Case 3: Child (finer) neighbor
                if (child_idx < numNodes)
                {
                    Node cNode = nodesBuffer[child_idx];
                    float dx_k = exp2((float)cNode.layer);
                    
                    // Distance between cell i center and child cell k center
                    float distance = max(0.5 * (dx_i + dx_k), 1e-6);
                    
                    // Weight: child's portion of face area divided by distance
                    float w = childFaceArea / distance;
                    
                    diagonal_sum += w;
                }
                // Case 4: Dirichlet boundary
                else if (child_idx == numNodes)
                {
                    float dx_k = dx_i * 0.5;
                    float distance = max(0.5 * (dx_i + dx_k), 1e-6);
                    
                    // Weight: child's portion of face area divided by distance
                    float w = childFaceArea / distance;
                    
                    diagonal_sum += w;
                }
            }
        }
    }

    // The diagonal is the negative sum (since Laplacian is -L)
    // For Jacobi preconditioning, we use the absolute value (magnitude)
    // to match the Python implementation: A_diag = sum(abs(A_off))
    float outVal = diagonal_sum; // Store positive value (absolute value of diagonal)
    
    if (!isfinite(outVal)) outVal = 0.0;
    diagonalBuffer[i] = outVal;
}

// KERNEL 5: Apply Jacobi Preconditioner
// Computes z = D^-1 * r where D is the diagonal of the Laplacian matrix A
// z[i] = r[i] / (diagonal[i] + epsilon)
[numthreads(512, 1, 1)]
void ApplyJacobi(uint3 id : SV_DispatchThreadID)
{
    uint i = id.x;
    if (i >= numNodes) return;

    float r_val = xBuffer[i];
    if (!isfinite(r_val)) r_val = 0.0;

    float diag_val = diagonalBuffer[i];
    if (!isfinite(diag_val)) diag_val = 0.0;

    // Jacobi: z = D^-1 * r
    // Add small epsilon to avoid division by zero
    float epsilon = 1e-10;
    float z_val = r_val / (diag_val + epsilon);

    if (!isfinite(z_val)) z_val = 0.0;
    yBuffer[i] = z_val;
}