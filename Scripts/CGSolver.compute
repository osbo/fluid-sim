// High-level shader language
#pragma kernel ApplyMatrixAndDot
#pragma kernel BuildMatrixA
#pragma kernel Axpy
#pragma kernel Scale
#pragma kernel CalculateDivergence
#pragma kernel DotProduct
#pragma kernel ApplySparseGAndDot
#pragma kernel ApplySparseGT
#pragma kernel ClearBufferFloat
#pragma kernel PrecomputeIndices
#pragma kernel ApplyJacobi // Keep this
#pragma kernel GlobalReduceSum

#include "Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl"

struct faceVelocities
{
    float left; float right; float bottom; float top; float front; float back;
};

struct Node
{
    float3 position; float3 velocity; faceVelocities velocities;
    float mass; uint layer; uint mortonCode; uint active;
};

// --- Buffers ---
RWStructuredBuffer<Node> nodesBuffer;
RWStructuredBuffer<float> divergenceBuffer;
RWStructuredBuffer<uint> neighborsBuffer;
StructuredBuffer<uint> reverseNeighborsBuffer;

RWStructuredBuffer<float> pBuffer; 
RWStructuredBuffer<float> ApBuffer;
RWStructuredBuffer<float> xBuffer;
RWStructuredBuffer<float> yBuffer;

// Neural / Preconditioner Buffers
RWStructuredBuffer<float> matrixGBuffer;
RWStructuredBuffer<float> zBuffer;
RWStructuredBuffer<uint> scatterIndicesBuffer;

// NEW: The Laplacian Matrix A [N * 25]
// Slot 0 = Diagonal, Slots 1-24 = Neighbors 0-23
// Ensure this matches the C# name
RWStructuredBuffer<float> matrixABuffer; 

uint numNodes;
float a; 
float deltaTime;
uint reductionCount;

// Shared memory for dot product reduction (used by DotProduct)
groupshared float s_data[512];

// --- KERNEL: BUILD MATRIX A ---
// Computes the Laplacian weights once per frame
[numthreads(256, 1, 1)]
void BuildMatrixA(uint3 id : SV_DispatchThreadID)
{
    uint i = id.x;
    if (i >= numNodes) return;

    Node node = nodesBuffer[i];
    float dx_i = exp2((float)node.layer);
    float diagonal = 0.0;
    
    // Initialize off-diagonals to 0
    // SoA layout: matrixABuffer[slot * numNodes + index]
    for(int k=0; k<24; k++) {
        matrixABuffer[(k + 1) * numNodes + i] = 0.0;
    }

    // Loop over 6 faces
    for (int d = 0; d < 6; d++)
    {
        uint faceSlotBase = d * 4;
        uint neighbor_idx = neighborsBuffer[faceSlotBase * numNodes + i];

        // Case 1: True Boundary (Neumann) - No contribution
        if (neighbor_idx == numNodes + 1) { continue; }

        if (neighbor_idx < numNodes && nodesBuffer[neighbor_idx].layer >= node.layer)
        {
            // Case 2: Same-layer or Parent
            // We cannot access neighborsBuffer here for properties, must read nodesBuffer
            Node nNode = nodesBuffer[neighbor_idx];
            float dx_j = exp2((float)nNode.layer);
            
            float distance = max(0.5 * (dx_i + dx_j), 1e-6);
            float faceArea = dx_i * dx_i;
            float weight = faceArea / distance;

            // Apply time step scaling immediately
            weight *= deltaTime;

            // Laplacian construction:
            // out = sum(w * p_i) - sum(w * p_j)
            // Diagonal += weight
            // Off-Diagonal = -weight
            
            diagonal += weight;
            
            // Find which slot this neighbor occupies (it's the first slot of the face)
            matrixABuffer[(faceSlotBase + 1) * numNodes + i] = -weight;
        }
        else
        {
            // Case 3/4: Child neighbors or Dirichlet
            float faceArea = dx_i * dx_i;
            float childFaceArea = faceArea / 4.0;
            
            for (int k = 0; k < 4; k++)
            {
                uint child_idx = neighborsBuffer[(faceSlotBase + k) * numNodes + i];
                uint slot = faceSlotBase + k;

                if (child_idx < numNodes) // Case 3: Child
                {
                    Node cNode = nodesBuffer[child_idx];
                    float dx_k = exp2((float)cNode.layer);
                    float distance = max(0.5 * (dx_i + dx_k), 1e-6);
                    float weight = (childFaceArea / distance) * deltaTime;

                    diagonal += weight;
                    matrixABuffer[(slot + 1) * numNodes + i] = -weight;
                }
                else if (child_idx == numNodes) // Case 4: Dirichlet (Air)
                {
                    // Ghost value is 0.0. 
                    // Term is w * (p_i - p_ghost) = w * p_i
                    // Contributes to diagonal only.
                    float dx_k = dx_i * 0.5;
                    float distance = max(0.5 * (dx_i + dx_k), 1e-6);
                    float weight = (childFaceArea / distance) * deltaTime;
                    
                    diagonal += weight;
                    // Off-diagonal is 0 because p_ghost is 0
                }
            }
        }
    }

    if (!isfinite(diagonal)) diagonal = 0.0;
    matrixABuffer[0 * numNodes + i] = diagonal; // Slot 0 is Diagonal
}

// --- KERNEL: APPLY MATRIX A + DOT ---
// Standard Sparse Matrix-Vector Multiply: Ap = A * p
// + Fused Dot Product (p . Ap)
groupshared float s_dot_reduction[256];
groupshared float s_p_values[256]; // Cache for vector p

[numthreads(256, 1, 1)]
void ApplyMatrixAndDot(uint3 Gid : SV_GroupID, uint3 Gtid : SV_GroupThreadID, uint3 id : SV_DispatchThreadID)
{
    uint i = id.x;
    uint li = Gtid.x;
    uint group_start_idx = Gid.x * 256;

    // --- Phase 1: Collaborative Load p into LDS ---
    if (i < numNodes) {
        s_p_values[li] = pBuffer[i];
    } else {
        s_p_values[li] = 0.0;
    }
    GroupMemoryBarrierWithGroupSync();

    float Ap_val = 0.0;
    float p_i = 0.0;

    if (i < numNodes)
    {
        p_i = s_p_values[li];
        if (!isfinite(p_i)) p_i = 0.0;

        // 1. Diagonal
        float diag = matrixABuffer[i]; // Slot 0
        float sum = diag * p_i;

        // 2. Off-Diagonals (Neighbors)
        // Iterate all 24 slots
        for (int k = 0; k < 24; k++)
        {
            uint neighbor_idx = neighborsBuffer[k * numNodes + i];

            // Optimization: If weight is 0, skip (though reading weight costs bandwidth)
            // Better Optimization: Check neighbor validity first
            if (neighbor_idx < numNodes)
            {
                float weight = matrixABuffer[(k + 1) * numNodes + i];
                float p_neighbor;

                // LDS Cache Check
                if (neighbor_idx >= group_start_idx && neighbor_idx < group_start_idx + 256)
                {
                    p_neighbor = s_p_values[neighbor_idx - group_start_idx];
                }
                else
                {
                    p_neighbor = pBuffer[neighbor_idx];
                }

                if (!isfinite(p_neighbor)) p_neighbor = 0.0;
                sum += weight * p_neighbor;
            }
        }

        if (!isfinite(sum)) sum = 0.0;
        Ap_val = sum;
        ApBuffer[i] = Ap_val;
    }

    // --- Phase 2: Fused Dot Product (p . Ap) ---
    float product = p_i * Ap_val;
    if (!isfinite(product)) product = 0.0;

    s_dot_reduction[li] = product;
    GroupMemoryBarrierWithGroupSync();

    // Standard Reduction
    for (uint s = 128; s > 0; s >>= 1) {
        if (li < s) {
            s_dot_reduction[li] += s_dot_reduction[li + s];
        }
        GroupMemoryBarrierWithGroupSync();
    }

    if (li == 0) {
        divergenceBuffer[Gid.x] = s_dot_reduction[0];
    }
}

// ... [REMAINING KERNELS: CalculateDivergence, Axpy, Scale, etc. UNCHANGED] ...
// Copy the rest of the original file from "void CalculateDivergence..." downwards.
// IMPORTANT: Keep PrecomputeIndices, ApplySparseGT, etc., for the preconditioner logic.

[numthreads(512, 1, 1)]
void CalculateDivergence(uint3 id : SV_DispatchThreadID)
{
    uint i = id.x;
    if (i >= numNodes) return;
    Node node = nodesBuffer[i];
    faceVelocities v = node.velocities;

    // Calculate Net Flux of Velocity
    float dx = exp2((float)node.layer);
    float faceArea = dx * dx;
    
    // Net flux leaving the cell
    float netFlux = (v.right - v.left + v.top - v.bottom + v.back - v.front) * faceArea;
    // We solve Ap = b. If A is negative Laplacian (positive definite), b should be negative Divergence.
    float outVal = -netFlux;
    if (!isfinite(outVal)) outVal = 0.0;
    divergenceBuffer[i] = outVal;
}

[numthreads(512, 1, 1)]
void Scale(uint3 id : SV_DispatchThreadID) {
    uint i = id.x;
    if (i >= numNodes) return;
    float y_val = yBuffer[i];
    yBuffer[i] = a * y_val;
}

[numthreads(512, 1, 1)]
void Axpy(uint3 id : SV_DispatchThreadID) {
    uint i = id.x;
    if (i >= numNodes) return;
    float x_val = xBuffer[i];
    float y_val = yBuffer[i];
    yBuffer[i] = a * x_val + y_val;
}

[numthreads(512, 1, 1)]
void DotProduct(uint3 Gid : SV_GroupID, uint3 Gtid : SV_GroupThreadID, uint3 id : SV_DispatchThreadID) {
    uint i = id.x;
    uint li = Gtid.x;
    float x_val = (i < numNodes) ? xBuffer[i] : 0.0;
    float y_val = (i < numNodes) ? yBuffer[i] : 0.0;
    if (!isfinite(x_val)) x_val = 0.0;
    if (!isfinite(y_val)) y_val = 0.0;
    float product = x_val * y_val;
    if (!isfinite(product)) product = 0.0;
    s_data[li] = product;
    GroupMemoryBarrierWithGroupSync();
    for (uint s = 256; s > 0; s >>= 1) {
        if (li < s) s_data[li] += s_data[li + s];
        GroupMemoryBarrierWithGroupSync();
    }
    if (li == 0) divergenceBuffer[Gid.x] = s_data[0];
}

// ... [Include PrecomputeIndices, ApplySparseGT, ApplySparseGAndDot, ClearBufferFloat, ApplyJacobi, GlobalReduceSum from original file] ...
// (These are needed if you toggle the preconditioner back on later)
[numthreads(512, 1, 1)]
void PrecomputeIndices(uint3 id : SV_DispatchThreadID) {
    uint i = id.x;
    if (i >= numNodes) return;
    for (int n = 0; n < 24; n++) {
        uint k = neighborsBuffer[n * numNodes + i];
        scatterIndicesBuffer[n * numNodes + i] = (k < numNodes) ? k : 0xFFFFFFFF;
    }
}

groupshared float s_x[512];
[numthreads(512, 1, 1)]
void ApplySparseGT(uint3 Gid : SV_GroupID, uint3 Gtid : SV_GroupThreadID, uint3 id : SV_DispatchThreadID) {
    uint i = id.x; uint li = Gtid.x; uint group_start_idx = Gid.x * 512;
    if (i < numNodes) s_x[li] = xBuffer[i]; else s_x[li] = 0.0;
    GroupMemoryBarrierWithGroupSync();
    if (i >= numNodes) return;
    float r_i = s_x[li]; if (!isfinite(r_i)) r_i = 0.0;
    float g_diag = matrixGBuffer[i]; if (!isfinite(g_diag)) g_diag = 0.0;
    float sum = g_diag * r_i;
    for (int n = 0; n < 24; n++) {
        uint k = scatterIndicesBuffer[n * numNodes + i];
        if (k != 0xFFFFFFFF) {
            uint revSlot = reverseNeighborsBuffer[i * 24 + n];
            if (revSlot < 24) {
                float g_ki = matrixGBuffer[(1 + revSlot) * numNodes + k];
                if (!isfinite(g_ki)) g_ki = 0.0;
                float r_k;
                if (k >= group_start_idx && k < group_start_idx + 512) r_k = s_x[k - group_start_idx];
                else r_k = xBuffer[k];
                if (!isfinite(r_k)) r_k = 0.0;
                sum += g_ki * r_k;
            }
        }
    }
    if (!isfinite(sum)) sum = 0.0;
    zBuffer[i] = sum;
}

groupshared float s_precon_dot[256];
groupshared float s_z_values[256];
[numthreads(256, 1, 1)]
void ApplySparseGAndDot(uint3 Gid : SV_GroupID, uint3 Gtid : SV_GroupThreadID, uint3 id : SV_DispatchThreadID) {
    uint i = id.x; uint li = Gtid.x; uint group_start_idx = Gid.x * 256;
    if (i < numNodes) s_z_values[li] = zBuffer[i];
    GroupMemoryBarrierWithGroupSync();
    float z_val = 0.0; float r_val = 0.0;
    if (i < numNodes) {
        float u_val = s_z_values[li]; if (!isfinite(u_val)) u_val = 0.0;
        r_val = xBuffer[i]; if (!isfinite(r_val)) r_val = 0.0;
        float z_sum = 0.0;
        float g_diag = matrixGBuffer[i]; if (!isfinite(g_diag)) g_diag = 0.0;
        z_sum += u_val * g_diag;
        for(int k=0; k<24; k++) {
            uint n_idx = neighborsBuffer[k * numNodes + i];
            if(n_idx < numNodes) {
                float g_coeff = matrixGBuffer[(k + 1) * numNodes + i];
                if (!isfinite(g_coeff)) g_coeff = 0.0;
                float u_neighbor;
                if (n_idx >= group_start_idx && n_idx < group_start_idx + 256) u_neighbor = s_z_values[n_idx - group_start_idx];
                else u_neighbor = zBuffer[n_idx];
                if (!isfinite(u_neighbor)) u_neighbor = 0.0;
                z_sum += g_coeff * u_neighbor;
            }
        }
        z_sum += 1e-4 * r_val;
        if (!isfinite(z_sum)) z_sum = 0.0;
        z_val = z_sum;
        yBuffer[i] = z_val;
    }
    float product = r_val * z_val;
    if (!isfinite(product)) product = 0.0;
    s_precon_dot[li] = product;
    GroupMemoryBarrierWithGroupSync();
    for (uint s = 128; s > 0; s >>= 1) {
        if (li < s) s_precon_dot[li] += s_precon_dot[li + s];
        GroupMemoryBarrierWithGroupSync();
    }
    if (li == 0) divergenceBuffer[Gid.x] = s_precon_dot[0];
}

[numthreads(512, 1, 1)]
void ClearBufferFloat(uint3 id : SV_DispatchThreadID) {
    if (id.x < numNodes) zBuffer[id.x] = 0.0;
}

// --- REPLACED KERNEL: ApplyJacobi ---
// Uses the pre-calculated diagonal from matrixABuffer (Slot 0)
[numthreads(512, 1, 1)]
void ApplyJacobi(uint3 id : SV_DispatchThreadID) {
    uint i = id.x;
    if (i >= numNodes) return;

    float r_val = xBuffer[i]; 
    if (!isfinite(r_val)) r_val = 0.0;

    // READ FROM MATRIX A (Slot 0 is Diagonal)
    float diag_val = matrixABuffer[0 * numNodes + i]; 
    
    // Safety check for zero / NaN diagonal
    if (!isfinite(diag_val)) diag_val = 0.0;
    
    // z = D^-1 * r
    float z_val = r_val / (diag_val + 1e-10);
    
    if (!isfinite(z_val)) z_val = 0.0;
    yBuffer[i] = z_val;
}

[numthreads(64, 1, 1)]
void GlobalReduceSum(uint3 id : SV_DispatchThreadID) {
    if (id.x != 0) return;
    uint count = min(reductionCount, 2048); 
    float total = 0.0;
    for(uint i = 0; i < count; i++) total += divergenceBuffer[i];
    divergenceBuffer[0] = total;
}