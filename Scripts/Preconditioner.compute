#pragma kernel ComputeFeatures
#pragma kernel ComputeQKV
#pragma kernel ComputeAttention
#pragma kernel ComputeFFN
#pragma kernel PredictHead

// --- STRUCTS & CONSTANTS ---
struct faceVelocities { float left, right, bottom, top, front, back; };
struct Node {
    float3 position; float3 velocity; faceVelocities velocities;
    float mass; uint layer; uint mortonCode; uint active;
};

// --- BUFFERS ---
StructuredBuffer<Node> nodesBuffer;
StructuredBuffer<uint> neighborsBuffer;

// Weights
StructuredBuffer<float> weightsFeatureProj;
StructuredBuffer<float> biasFeatureProj;
StructuredBuffer<float> weightsLayerEmbed;
StructuredBuffer<float> weightsPosEmbed;

// Transformer Layer Weights
StructuredBuffer<float> w_attn_in;  // [128 * 384]
StructuredBuffer<float> b_attn_in;  // [384]
StructuredBuffer<float> w_attn_out; // [128 * 128]
StructuredBuffer<float> b_attn_out; // [128]
StructuredBuffer<float> w_norm1;    // [128]
StructuredBuffer<float> b_norm1;    // [128]
StructuredBuffer<float> w_ffn1;     // [128 * 256]
StructuredBuffer<float> b_ffn1;     // [256]
StructuredBuffer<float> w_ffn2;     // [256 * 128]
StructuredBuffer<float> b_ffn2;     // [128]
StructuredBuffer<float> w_norm2;    // [128]
StructuredBuffer<float> b_norm2;    // [128]

// Head Weights
StructuredBuffer<float> w_normOut;
StructuredBuffer<float> b_normOut;
StructuredBuffer<float> w_head;     // [128 * 25]
StructuredBuffer<float> b_head;     // [25]

// Data Buffers
RWStructuredBuffer<float> tokenBuffer;      // [MaxNodes * 128]
RWStructuredBuffer<float> tokenBufferOut;   // [MaxNodes * 128]
RWStructuredBuffer<float> bufferQ;          // [MaxNodes * 128]
RWStructuredBuffer<float> bufferK;          // [MaxNodes * 128]
RWStructuredBuffer<float> bufferV;          // [MaxNodes * 128]
RWStructuredBuffer<float> bufferAttn;       // [MaxNodes * 128]
RWStructuredBuffer<float> matrixGBuffer;    // [NumNodes * 25]

uint numNodes;
uint maxNodes;

// --- HELPERS ---
float GetDx(uint layer) { return pow(2.0, (float)layer); }

float Gelu(float x) {
    return x * 0.5 * (1.0 + tanh(0.79788 * (x + 0.044715 * x * x * x)));
}

// --- MACRO FOR REDUCTION ---
// Replaces the helper function to avoid passing shared memory arrays
#define REDUCE_SUM_128(tid, val, s_mem) \
    s_mem[tid] = val; \
    GroupMemoryBarrierWithGroupSync(); \
    if (tid < 64) s_mem[tid] += s_mem[tid + 64]; GroupMemoryBarrierWithGroupSync(); \
    if (tid < 32) s_mem[tid] += s_mem[tid + 32]; GroupMemoryBarrierWithGroupSync(); \
    if (tid < 16) s_mem[tid] += s_mem[tid + 16]; GroupMemoryBarrierWithGroupSync(); \
    if (tid < 8)  s_mem[tid] += s_mem[tid + 8];  GroupMemoryBarrierWithGroupSync(); \
    if (tid < 4)  s_mem[tid] += s_mem[tid + 4];  GroupMemoryBarrierWithGroupSync(); \
    if (tid < 2)  s_mem[tid] += s_mem[tid + 2];  GroupMemoryBarrierWithGroupSync(); \
    if (tid < 1)  s_mem[tid] += s_mem[tid + 1];  GroupMemoryBarrierWithGroupSync(); \
    val = s_mem[0]; \
    GroupMemoryBarrierWithGroupSync();

// --- KERNEL 1: FEATURES ---
// FIXED: Changed to 512 to match C# windowGroups dispatch (N/512 groups * 512 threads = N)
[numthreads(512, 1, 1)] 
void ComputeFeatures(uint3 id : SV_DispatchThreadID)
{
    uint i = id.x;
    if (i >= maxNodes) return;

    // 1. Feature Extraction (You correctly pasted this!)
    float feats[58]; 
    for(int k=0; k<58; k++) feats[k] = 0.0;
    
    if (i < numNodes) {
        Node n = nodesBuffer[i];
        feats[0] = n.position.x / 1024.0;
        feats[1] = n.position.y / 1024.0;
        feats[2] = n.position.z / 1024.0;

        float A_diag = 0.0;
        float dx = GetDx(n.layer);
        uint n_base = i * 24;
        int wall_starts[6] = {0, 4, 8, 12, 16, 20};

        for (int f = 0; f < 6; f++) {
            uint w_idx = neighborsBuffer[n_base + wall_starts[f]];
            if (w_idx == numNodes + 1) feats[3 + f] = 1.0; // Wall flag
            
            bool isCoarseOrSame = false;
            uint slot0_idx = neighborsBuffer[n_base + f * 4];
            if (slot0_idx < numNodes) {
                Node n0 = nodesBuffer[slot0_idx];
                if (n0.layer >= n.layer) isCoarseOrSame = true;
            }

            for (int s = 0; s < 4; s++) {
                int local_idx = f * 4 + s;
                if (isCoarseOrSame && s > 0) continue;

                uint n_idx = neighborsBuffer[n_base + local_idx];
                if (n_idx == numNodes) feats[9 + local_idx] = 1.0; // Air flag
                
                float weight = 0.0;
                if (n_idx < numNodes) {
                    Node nb = nodesBuffer[n_idx];
                    float dist = max(0.5 * (dx + GetDx(nb.layer)), 1e-6);
                    float area = (isCoarseOrSame) ? (dx*dx) : (dx*dx*0.25);
                    weight = area / dist;
                    feats[34 + local_idx] = -weight; // A_off
                } else if (n_idx == numNodes) { 
                    weight = (dx*dx*0.25) / max(0.5 * dx, 1e-6);
                }
                A_diag += weight;
            }
        }
        feats[33] = A_diag;
    }

    uint layer = (i < numNodes) ? nodesBuffer[i].layer : 0;
    uint win_pos = i % 512;
    
    // 2. Linear Projection
    for (int d = 0; d < 128; d++) {
        float val = biasFeatureProj[d];
        for (int f = 0; f < 58; f++) val += feats[f] * weightsFeatureProj[f * 128 + d];
        val += weightsLayerEmbed[layer * 128 + d];
        val += weightsPosEmbed[win_pos * 128 + d];
        tokenBuffer[i * 128 + d] = val;
    }
}

// --- KERNEL 2: QKV (Node Parallel) ---
groupshared float sm_x[128]; // Used for broadcast
groupshared float s_reduce[128]; // Used for reduction

[numthreads(128, 1, 1)]
void ComputeQKV(uint3 id : SV_DispatchThreadID, uint3 groupID : SV_GroupID, uint groupIndex : SV_GroupIndex)
{
    uint nodeIdx = groupID.x;
    uint dim = groupIndex; // 0..127
    if (nodeIdx >= maxNodes) return;

    // 1. Load & Norm
    float val = tokenBuffer[nodeIdx * 128 + dim];
    
    // Mean Reduction
    float sum = val;
    REDUCE_SUM_128(dim, sum, s_reduce);
    float mean = sum / 128.0;
    
    // Var Reduction
    float diff = val - mean;
    float sqDiff = diff * diff;
    REDUCE_SUM_128(dim, sqDiff, s_reduce); // Reuses s_reduce safely
    
    float invStd = rsqrt(sqDiff / 128.0 + 1e-5);
    float normVal = diff * invStd * w_norm1[dim] + b_norm1[dim];
    
    // Store for broadcast
    sm_x[dim] = normVal;
    GroupMemoryBarrierWithGroupSync();

    // 2. Compute Q, K, V
    float q = b_attn_in[dim];
    float k_val = b_attn_in[128 + dim];
    float v_val = b_attn_in[256 + dim];

    for (int k = 0; k < 128; k++) {
        float x_k = sm_x[k]; 
        q     += x_k * w_attn_in[k * 384 + dim];
        k_val += x_k * w_attn_in[k * 384 + 128 + dim];
        v_val += x_k * w_attn_in[k * 384 + 256 + dim];
    }

    bufferQ[nodeIdx * 128 + dim] = q;
    bufferK[nodeIdx * 128 + dim] = k_val;
    bufferV[nodeIdx * 128 + dim] = v_val;
}

// --- KERNEL 3: ATTENTION (Window + Head Parallel) ---
// Note: No WaveActiveSum needed here, strictly shared memory tiling.
groupshared float lds_K[512]; // 16 tokens * 32 dims
groupshared float lds_V[512]; 

[numthreads(512, 1, 1)]
void ComputeAttention(uint3 groupID : SV_GroupID, uint3 groupThreadID : SV_GroupThreadID, uint3 id : SV_DispatchThreadID)
{
    uint windowIdx = groupID.x;
    uint headIdx = groupID.y; // 0..3
    uint localId = groupThreadID.x; // 0..511 (Token inside window)
    
    uint globalTokenIdx = windowIdx * 512 + localId;
    
    uint headOffset = headIdx * 32; 
    
    // Load Q locally (registers)
    float q_local[32];
    bool valid = globalTokenIdx < maxNodes;
    
    if (valid) {
        for(int d=0; d<32; d++) {
            q_local[d] = bufferQ[globalTokenIdx * 128 + headOffset + d];
        }
    } else {
        for(int d=0; d<32; d++) q_local[d] = 0;
    }

    float m = -1e9;
    float d = 0;
    float res[32]; 
    for(int z=0; z<32; z++) res[z] = 0;
    
    float scale = 0.17677; // 1/sqrt(32)
    uint windowStart = windowIdx * 512;
    
    // Tiled Processing: 32 tiles * 16 tokens = 512 total
    for(int tile = 0; tile < 32; tile++)
    {
        uint tileStartToken = tile * 16; 
        
        // Cooperative Load K/V
        uint loadIdx = localId; 
        if (loadIdx < 512) {
            uint t_token = loadIdx / 32; 
            uint t_dim = loadIdx % 32;   
            uint g_node = windowStart + tileStartToken + t_token;
            
            if (g_node < maxNodes) {
                lds_K[loadIdx] = bufferK[g_node * 128 + headOffset + t_dim];
                lds_V[loadIdx] = bufferV[g_node * 128 + headOffset + t_dim];
            } else {
                lds_K[loadIdx] = 0;
                lds_V[loadIdx] = 0;
            }
        }
        GroupMemoryBarrierWithGroupSync();

        // Compute Attention for this tile
        for(int j=0; j<16; j++) {
            float score = 0;
            uint k_base = j * 32;
            for(int dim=0; dim<32; dim++) {
                score += q_local[dim] * lds_K[k_base + dim];
            }
            score *= scale;
            
            float m_prev = m;
            m = max(m_prev, score);
            float e_score = exp(score - m);
            float e_prev = exp(m_prev - m);
            
            d = d * e_prev + e_score;
            
            for(int d_v=0; d_v<32; d_v++) {
                res[d_v] = res[d_v] * e_prev + lds_V[k_base + d_v] * e_score;
            }
        }
        GroupMemoryBarrierWithGroupSync();
    }
    
    if (!valid) return;

    float inv_d = 1.0 / (d + 1e-6);
    for(int d_i=0; d_i<32; d_i++) {
        bufferAttn[globalTokenIdx * 128 + headOffset + d_i] = res[d_i] * inv_d;
    }
}

// --- KERNEL 4: FFN (Node Parallel) ---
groupshared float sm_input[128];
groupshared float sm_hidden[256];
groupshared float s_reduce_ffn[128]; // Dedicated reduction buffer

[numthreads(128, 1, 1)]
void ComputeFFN(uint3 id : SV_DispatchThreadID, uint3 groupID : SV_GroupID, uint groupIndex : SV_GroupIndex)
{
    uint nodeIdx = groupID.x;
    uint dim = groupIndex; // 0..127
    if (nodeIdx >= maxNodes) return;

    // --- STEP A: Linear Projection (Attn Out) ---
    // Cooperative load Attn Output
    sm_input[dim] = bufferAttn[nodeIdx * 128 + dim];
    GroupMemoryBarrierWithGroupSync();
    
    // Apply W_attn_out
    float proj_val = b_attn_out[dim];
    for(int k=0; k<128; k++) {
        proj_val += sm_input[k] * w_attn_out[k * 128 + dim];
    }
    
    // Residual 1
    float x_in = tokenBuffer[nodeIdx * 128 + dim];
    float x_res1 = x_in + proj_val;
    
    // --- Norm 2 ---
    float sum = x_res1;
    REDUCE_SUM_128(dim, sum, s_reduce_ffn);
    float mean = sum / 128.0;
    
    float diff = x_res1 - mean;
    float sqDiff = diff * diff;
    REDUCE_SUM_128(dim, sqDiff, s_reduce_ffn);
    float invStd = rsqrt(sqDiff / 128.0 + 1e-5);
    
    float x_norm = diff * invStd * w_norm2[dim] + b_norm2[dim];
    
    // Store Normalized Input
    sm_input[dim] = x_norm;
    GroupMemoryBarrierWithGroupSync();
    
    // --- STEP B: FFN Layer 1 (128 -> 256) ---
    // Thread computes 2 values
    float h1 = b_ffn1[dim];
    for(int i=0; i<128; i++) h1 += sm_input[i] * w_ffn1[i * 256 + dim];
    sm_hidden[dim] = Gelu(h1);
    
    float h2 = b_ffn1[dim + 128];
    for(int j=0; j<128; j++) h2 += sm_input[j] * w_ffn1[j * 256 + (dim + 128)];
    sm_hidden[dim + 128] = Gelu(h2);
    
    GroupMemoryBarrierWithGroupSync();
    
    // --- STEP C: FFN Layer 2 (256 -> 128) + Residual ---
    float ffn_out = b_ffn2[dim];
    for(int h=0; h<256; h++) {
        ffn_out += sm_hidden[h] * w_ffn2[h * 128 + dim];
    }
    
    float final_val = x_res1 + ffn_out;
    tokenBufferOut[nodeIdx * 128 + dim] = final_val;
}

// --- KERNEL 5: PREDICT HEAD ---
groupshared float s_reduce_head[128];

[numthreads(128, 1, 1)]
void PredictHead(uint3 groupID : SV_GroupID, uint groupIndex : SV_GroupIndex)
{
    uint nodeIdx = groupID.x;
    uint dim = groupIndex;
    if (nodeIdx >= numNodes) return;
    
    float val = tokenBufferOut[nodeIdx * 128 + dim];
    
    // Norm Out
    float sum = val;
    REDUCE_SUM_128(dim, sum, s_reduce_head);
    float mean = sum / 128.0;
    
    float diff = val - mean;
    float sqDiff = diff * diff;
    REDUCE_SUM_128(dim, sqDiff, s_reduce_head);
    float invStd = rsqrt(sqDiff / 128.0 + 1e-5);
    
    float x_norm = diff * invStd * w_normOut[dim] + b_normOut[dim];
    
    // Store for projection
    // We reuse s_reduce_head to broadcast the normalized vector
    s_reduce_head[dim] = x_norm;
    GroupMemoryBarrierWithGroupSync();
    
    // Linear 128 -> 25
    if (dim < 25) {
        float outVal = b_head[dim];
        for(int k=0; k<128; k++) {
            outVal += s_reduce_head[k] * w_head[k * 25 + dim];
        }
        matrixGBuffer[nodeIdx * 25 + dim] = outVal;
    }
}
